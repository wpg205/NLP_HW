{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(42)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/wpg205'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/scratch/wpg205')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.append(PAD_IDX)\n",
    "    ordered_words_ft.append(UNK_IDX)\n",
    "    for t, line in enumerate(f):\n",
    "        i = t + 2 \n",
    "        if t >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "\n",
    "loaded_embeddings_ft_torch = torch.tensor(loaded_embeddings_ft, requires_grad = True)\n",
    "del loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "test\n",
      "is\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(ordered_words_ft[words_ft['the']])\n",
    "print(ordered_words_ft[words_ft['test']])\n",
    "print(ordered_words_ft[words_ft['is']])\n",
    "print(ordered_words_ft[words_ft['passed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = pd.read_csv('snli_train.tsv', sep='\\t') \n",
    "#snli_train = snli_train.iloc[:10000,:]\n",
    "snli_train['sentence1'] = snli_train['sentence1'].str.split()\n",
    "snli_train['sentence2'] = snli_train['sentence2'].str.split()\n",
    "\n",
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t') \n",
    "#snli_val = snli_val.iloc[:10000,:]\n",
    "snli_val['sentence1'] = snli_val['sentence1'].str.split()\n",
    "snli_val['sentence2'] = snli_val['sentence2'].str.split()\n",
    "\n",
    "snli_train['label'].replace('neutral',0, inplace=True)\n",
    "snli_train['label'].replace('entailment',1, inplace = True)\n",
    "snli_train['label'].replace('contradiction',2, inplace = True)\n",
    "\n",
    "snli_val['label'].replace('neutral',0, inplace=True)\n",
    "snli_val['label'].replace('entailment',1, inplace = True)\n",
    "snli_val['label'].replace('contradiction',2, inplace = True)\n",
    "\n",
    "data_tup_train = zip(snli_train.sentence1,snli_train.sentence2,snli_train.label)\n",
    "data_tup_val = zip(snli_val.sentence1,snli_val.sentence2,snli_val.label)\n",
    "\n",
    "del snli_train\n",
    "del snli_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_sen_length = 82\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, char2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1,self.sent2,self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.sent1) == len(self.target_list))\n",
    "        assert (len(self.sent2) == len(self.target_list))\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        char_idx_1 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.sent1[key][:Max_sen_length]]\n",
    "        char_idx_2 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.sent2[key][:Max_sen_length]]\n",
    "        label = self.target_list[key]\n",
    "        return [char_idx_1, char_idx_2,len(char_idx_1),len(char_idx_2),label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    length_list_2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list.append(datum[2])\n",
    "        length_list_2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,Max_sen_length-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,Max_sen_length-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec1)\n",
    "        data_list_2.append(padded_vec2)\n",
    "        \n",
    "\n",
    "    ordering1 = np.linspace(0, len(data_list_1),len(data_list_1), endpoint = False)\n",
    "    ordering2 = np.linspace(0, len(data_list_2),len(data_list_2), endpoint = False)\n",
    "      \n",
    "    ind_dec_order = np.argsort(length_list)[::-1]\n",
    "    ind_dec_order_2 = np.argsort(length_list_2)[::-1]\n",
    "    \n",
    "    ordering1 = ordering1[ind_dec_order]\n",
    "    ordering2 = ordering2[ind_dec_order_2]\n",
    "        \n",
    "    data_list_1 = np.array(data_list_1)[ind_dec_order]\n",
    "    data_list_2 = np.array(data_list_2)[ind_dec_order_2]\n",
    "                \n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    length_list_2 = np.array(length_list_2)[ind_dec_order_2]\n",
    "    \n",
    "    \n",
    "    mask1 = np.array(data_list_1)\n",
    "    mask2 = np.array(data_list_2)\n",
    "    \n",
    "    mask1[mask1 > 0] = 1\n",
    "    mask2[mask2 > 0] = 1\n",
    "    \n",
    "    \n",
    "    ###need to mask padded values###\n",
    "    \n",
    "                \n",
    "    #label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(data_list_1)), torch.from_numpy(np.array(data_list_2)), \\\n",
    "            torch.LongTensor(length_list),torch.LongTensor(length_list_2), \\\n",
    "            torch.LongTensor(label_list), ordering1, ordering2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(data_tup_train, words_ft)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(data_tup_val, words_ft)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "del train_dataset\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size, bidirectional = True):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding = nn.Embedding.from_pretrained(loaded_embeddings_ft_torch, freeze = False).to(device)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional = True).to(device)\n",
    "        self.linear1 = nn.Linear(hidden_size*2*2, 100).to(device)\n",
    "        self.linear2 = nn.Linear(100, 50).to(device)\n",
    "        self.linear = nn.Linear(50, num_classes).to(device)\n",
    "        #self.linear = nn.Linear(hidden_size*2*2, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden.to(device)\n",
    "\n",
    "    def forward(self, x1,x2,lengths1,lengths2, ordering1, ordering2):\n",
    "        #setting mask\n",
    "        \"\"\"\"\"\"\n",
    "        m1 = torch.zeros([x1.size(0), x1.size(1)]).to(device)\n",
    "        m2 = torch.zeros([x2.size(0), x2.size(1)]).to(device)\n",
    "        \n",
    "        m1[x1 == 1] = 1\n",
    "        m2[x2 == 1] = 1\n",
    "    \n",
    "        #m1 = torch.from_numpy(m1)\n",
    "        #m2 = torch.from_numpy(m2)\n",
    "        \n",
    "        \n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = x1.size()\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding(x1.to(device))\n",
    "        embed2 = self.embedding(x2.to(device))\n",
    "        \n",
    "        m1 = m1.unsqueeze(-1).expand_as(embed1).to(device)\n",
    "        m2 = m2.unsqueeze(-1).expand_as(embed2).to(device)\n",
    "        embed1 = embed1.to(torch.float).to(device)\n",
    "        embed2 = embed2.to(torch.float).to(device)\n",
    "        m1 = m1.to(torch.float).to(device)\n",
    "        m2 = m2.to(torch.float).to(device)\n",
    "\n",
    "        embed1 = m1 * embed1+ (1-m1) * embed1.clone().detach()\n",
    "        embed2 = m2 * embed2+ (1-m2) * embed2.clone().detach()\n",
    "        \n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, lengths1.numpy(), batch_first=True)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, lengths2.numpy(), batch_first=True)\n",
    "        \n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size).to(device)\n",
    "        rnn_out1, self.hidden1 = self.rnn(embed1, self.hidden)\n",
    "        \n",
    "        # undo packing\n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size).to(device)\n",
    "        rnn_out2, self.hidden2 = self.rnn(embed2, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "        \"\"\"        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        rnn_out1, self.hidden1 = self.rnn(embed1, self.hidden)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        rnn_out2, self.hidden2 = self.rnn(embed2, self.hidden)\n",
    "        \"\"\"\n",
    "        reverse1 = np.argsort(ordering1)\n",
    "        reverse2 = np.argsort(ordering2)\n",
    "        #print(rnn_out1.size())\n",
    "        #Use hidden state rather than the last one\n",
    "        \n",
    "        \n",
    "        rnn_out1 = rnn_out1[reverse1,:,:].to(device)\n",
    "        rnn_out2 = rnn_out2[reverse2,:,:].to(device)\n",
    "        \n",
    "        #self.hidden1 = self.hidden1[:,reverse1,:]\n",
    "        #self.hidden2 = self.hidden2[:,reverse2,:]\n",
    "        #print(rnn_out1)\n",
    "        #print(rnn_out2.size())\n",
    "        \n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1).to(device)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1).to(device)\n",
    "        #rnn_out1 = rnn_out1[:,-1,:]\n",
    "        #rnn_out2 = rnn_out2[:,-1,:]\n",
    "        \n",
    "        rnn_out = torch.cat((rnn_out1,rnn_out2), 1).to(device)\n",
    "        #rnn_out = torch.cat((self.hidden1,self.hidden2), 2)\n",
    "\n",
    "        rnn_out = F.relu(self.linear1(rnn_out))\n",
    "        rnn_out = F.relu(self.linear2(rnn_out))\n",
    "        logits = self.linear(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size,kernel = 3):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding = nn.Embedding.from_pretrained(loaded_embeddings_ft_torch, freeze = False).to(device)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel, padding=1).to(device)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel, padding=1).to(device)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size*2, 200).to(device)\n",
    "        self.linear2 = nn.Linear(200, 100).to(device)\n",
    "        self.linear = nn.Linear(100, num_classes).to(device)\n",
    "\n",
    "    def forward(self, x1,x2,lengths1,lengths2, ordering1, ordering2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "        \"\"\"\n",
    "        mask1_size1 = mask1.size(0)\n",
    "        mask1_size2 = mask1.size(1)\n",
    "        \n",
    "        mask2_size1 = mask2.size(0)\n",
    "        mask2_size2 = mask2.size(1)\n",
    "        \n",
    "        mask1.unsqueeze_(-1)\n",
    "        mask1 = mask1.expand(mask1_size1,mask1_size2,200).to(torch.float).to(device)\n",
    "        \n",
    "        mask2.unsqueeze_(-1)\n",
    "        mask2 = mask2.expand(mask2_size1,mask2_size2,200).to(torch.float).to(device)\n",
    "        \"\"\"\n",
    "        \n",
    "        m1 = torch.zeros([x1.size(0), x1.size(1)]).to(device)\n",
    "        m2 = torch.zeros([x2.size(0), x2.size(1)]).to(device)\n",
    "        \n",
    "        m1[x1 == 1] = 1\n",
    "        m2[x2 == 1] = 1\n",
    "\n",
    "        embed1 = self.embedding(x1).to(device)\n",
    "        embed2 = self.embedding(x2).to(device)\n",
    "        \n",
    "        m1 = m1.unsqueeze(-1).expand_as(embed1)\n",
    "        m2 = m2.unsqueeze(-1).expand_as(embed2)\n",
    "        \n",
    "        embed1 = embed1.to(torch.float)\n",
    "        embed2 = embed2.to(torch.float)\n",
    "        \n",
    "        m1 = m1.to(torch.float)\n",
    "        m2 = m2.to(torch.float)\n",
    "\n",
    "        embed1 = m1 * embed1+ (1-m1) * embed1.clone().detach()\n",
    "        embed2 = m2 * embed2+ (1-m2) * embed2.clone().detach()        \n",
    "        \n",
    "        hidden = self.conv1(embed1.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        drop1  = nn.Dropout()\n",
    "        hidden = drop1(hidden)\n",
    "\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, -1, hidden.size(-1))\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        hidden = drop1(hidden)\n",
    "        \n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, -1, hidden.size(-1))\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        hidden = hidden.max(dim = 1)[0]\n",
    "        #hidden = torch.sum(hidden, dim=1)\n",
    "        #print(hidden.size())\n",
    "        \n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        hidden2 = drop1(hidden2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, -1, hidden2.size(-1))\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        \n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        hidden2 = drop1(hidden2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, -1, hidden2.size(-1))\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        hidden2 = hidden2.max(dim = 1)[0]\n",
    "        #hidden2 = torch.sum(hidden2, dim=1)\n",
    "        \n",
    "        \n",
    "        reverse1 = np.argsort(ordering1)\n",
    "        reverse2 = np.argsort(ordering2)\n",
    "    \n",
    "        cnn_out1 = hidden[reverse1,:]\n",
    "        cnn_out2 = hidden2[reverse2,:]\n",
    "        #print(cnn_out1.size())\n",
    "        #cnn_out1 = cnn_out1[:,-1]\n",
    "        #cnn_out2 = cnn_out2[:,-1]\n",
    "        #print(cnn_out1.size())\n",
    "        \n",
    "        cnn_out = torch.cat((cnn_out1,cnn_out2), 1)\n",
    "        \n",
    "        cnn_out = F.relu(self.linear1(cnn_out))\n",
    "        cnn_out = F.relu(self.linear2(cnn_out))\n",
    "        logits = self.linear(cnn_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data1,data2,lengths1,lengths2,labels,ordering1,ordering2 in loader:\n",
    "            outputs = model(data1.to(device),data2.to(device),lengths1.to(device),lengths2.to(device),\\\n",
    "                        ordering1,ordering2)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            total_loss += loss/labels.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_hidden(hidden_dim):\n",
    "    \n",
    "    model = GRU(emb_size=300, hidden_size=hidden_dim, num_layers=1, num_classes=3, vocab_size=words_to_load)\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 8 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    train_loss_vals = []\n",
    "    train_acc_vals = []\n",
    "    val_loss_vals = []\n",
    "    val_acc_vals = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data1,data2,lengths1,lengths2,labels,ordering1,ordering2) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data1,data2,lengths1,lengths2,ordering1,ordering2)\n",
    "            #print(outputs,labels)\n",
    "            loss = criterion(outputs,labels.to(device))\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                # validate\n",
    "                print(\"total lossing\")\n",
    "                val_acc, val_loss = test_model(val_loader, model)\n",
    "                train_acc, train_loss = test_model(train_loader, model)\n",
    "                train_loss_vals.append(train_loss)\n",
    "                train_acc_vals.append(train_acc)\n",
    "                val_loss_vals.append(val_loss)\n",
    "                val_acc_vals.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, total_step, val_acc, train_acc )) \n",
    "                \n",
    "    return train_loss_vals, train_acc_vals, val_loss_vals, val_acc_vals, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 50.4, Train Acc: 51.071\n",
      "total lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 53.3, Train Acc: 55.285\n",
      "total lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 58.2, Train Acc: 56.816\n",
      "total lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 58.7, Train Acc: 57.638\n",
      "total lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 59.0, Train Acc: 58.825\n",
      "total lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 58.2, Train Acc: 59.471\n",
      "total lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 59.7, Train Acc: 60.328\n",
      "total lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 59.5, Train Acc: 61.2\n",
      "total lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 62.4, Train Acc: 62.46\n",
      "total lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 61.1, Train Acc: 62.767\n",
      "total lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 60.7, Train Acc: 63.274\n",
      "total lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 61.1, Train Acc: 63.779\n",
      "total lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 64.1, Train Acc: 64.204\n",
      "total lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 63.5, Train Acc: 64.49\n",
      "total lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 63.7, Train Acc: 65.483\n",
      "total lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 63.5, Train Acc: 65.363\n",
      "total lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 65.9, Train Acc: 66.268\n",
      "total lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 66.3, Train Acc: 66.13\n",
      "total lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 64.7, Train Acc: 66.646\n",
      "total lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 66.6, Train Acc: 66.759\n",
      "total lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 64.9, Train Acc: 67.259\n",
      "total lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 66.1, Train Acc: 67.553\n",
      "total lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 66.9, Train Acc: 68.039\n",
      "total lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 65.1, Train Acc: 67.856\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "training_losses_RNN, training_accs_RNN, val_losses_RNN, val_accs_RNN, RNN_model = rnn_hidden(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(val_loader, RNN_model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mistakes(dataset, model):\n",
    "    \"\"\"\n",
    "    function to identify three correct and three incorrect\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "    correct_list = []\n",
    "    incorrect_list = []\n",
    "    true_label_cor = []\n",
    "    true_label_incor = []\n",
    "    model_label = []\n",
    "    model.eval()\n",
    "    for data1,data2,lengths1,lengths2,labels,ordering1,ordering2 in dataset:\n",
    "        outputs = F.softmax(model(data1,data2, lengths1,lengths2,ordering1,ordering2), dim=0)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        if predicted[0].item() == labels[0].item():\n",
    "            correct_list.append(num)\n",
    "            true_label_cor.append(labels[0].item())\n",
    "        else:\n",
    "            incorrect_list.append(num)\n",
    "            true_label_incor.append(labels[0].item())\n",
    "            \n",
    "        model_label.append(predicted[0].item())    \n",
    "        \n",
    "        if len(correct_list) > 3 and len(incorrect_list) > 310:\n",
    "            return correct_list, incorrect_list, true_label_cor, true_label_incor, model_label\n",
    "        \n",
    "        num += 1\n",
    "    return print('fail')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t') \n",
    "#snli_val = snli_val.iloc[:10000,:]\n",
    "snli_val_full = snli_val.copy()\n",
    "snli_val['sentence1'] = snli_val['sentence1'].str.split()\n",
    "snli_val['sentence2'] = snli_val['sentence2'].str.split()\n",
    "\n",
    "snli_val['label'].replace('neutral',0, inplace=True)\n",
    "snli_val['label'].replace('entailment',1, inplace = True)\n",
    "snli_val['label'].replace('contradiction',2, inplace = True)\n",
    "\n",
    "data_tup_val = zip(snli_val.sentence1,snli_val.sentence2,snli_val.label)\n",
    "\n",
    "val_dataset = VocabDataset(data_tup_val, words_ft)\n",
    "val_loader_final = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of people dressed in Santa Claus suits are looking towards an audience while a DJ runs a sound board and another person throws green balls into the air .\n",
      "A band plays at a beach party .\n",
      "labeled as neutral \n",
      "\n",
      "A female basketball player dribbling down court .\n",
      "A basketball player is destroying the ball .\n",
      "labeled as neutral \n",
      "\n",
      "A building that portrays beautiful architecture stands in the sunlight as somebody on a bike passes by .\n",
      "A bicyclist rides past an abandoned warehouse on a rainy day\n",
      "labeled as neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_list, incorrect_list, true_label_cor, true_label_incor, model_label = model_mistakes(val_loader_final, RNN_model)\n",
    "\n",
    "convert = {0:'neutral',1:'entailment', 2:'contradiction'}\n",
    "\n",
    "all_correct = []\n",
    "for y,x in enumerate(correct_list):\n",
    "    all_correct.append((snli_val_full.iloc[x,0], snli_val_full.iloc[x,1],snli_val_full.iloc[x,2],model_label[y]))\n",
    "\n",
    "    \n",
    "import operator\n",
    "all_correct.sort(key = operator.itemgetter(1))\n",
    "\n",
    "for x in range(0,3):\n",
    "    print(all_correct[x][0])\n",
    "    print(all_correct[x][1])\n",
    "    print(\"labeled as {}\".format(all_correct[x][2]), '\\n')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two young children in black clothing and padding perform a martial arts match where they compete against the other .\n",
      "2 young boys compete in martial arts .\n",
      "labeled as neutral, but in fact entailment \n",
      "\n",
      "The baby in a blue jean hat and sunglasses looks at the camera while being held .\n",
      "A baby looks at the camera .\n",
      "labeled as neutral, but in fact entailment \n",
      "\n",
      "A live band on a lawn jamming out .\n",
      "A band is practicing new tunes in the garage .\n",
      "labeled as neutral, but in fact contradiction \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_incorrect = []\n",
    "for y,x in enumerate(incorrect_list):\n",
    "    all_incorrect.append((snli_val_full.iloc[x,0], snli_val_full.iloc[x,1],snli_val_full.iloc[x,2],model_label[y]))\n",
    "\n",
    "all_incorrect.sort(key = operator.itemgetter(1))\n",
    "\n",
    "for x in range(0,3):\n",
    "    print(all_incorrect[x][0])\n",
    "    print(all_incorrect[x][1])\n",
    "    print(\"labeled as {}, but in fact {}\".format(convert[all_incorrect[x][3]],all_incorrect[x][2]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286803"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_params(model):\n",
    "    param_total = 0\n",
    "    for x in model.parameters():\n",
    "        if x.requires_grad:\n",
    "            store = 1\n",
    "            for y in range(len(x.size())):\n",
    "                store = store*x.size(y)\n",
    "            param_total += store\n",
    "    \n",
    "    #(these are untrained embeddings)\n",
    "    total_embeddings = 50001*300\n",
    "    param_total -= total_embeddings\n",
    "    return param_total\n",
    "\n",
    "total_params(RNN_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 59.5, Train Acc: 59.298\n",
      "lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 61.6, Train Acc: 62.195\n",
      "lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 59.9, Train Acc: 63.523\n",
      "lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 63.8, Train Acc: 65.02\n",
      "lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 62.6, Train Acc: 65.541\n",
      "lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 64.3, Train Acc: 66.319\n",
      "lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 65.2, Train Acc: 67.514\n",
      "lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 65.8, Train Acc: 68.046\n",
      "lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 65.2, Train Acc: 68.124\n",
      "lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 64.4, Train Acc: 68.444\n",
      "lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 67.5, Train Acc: 69.144\n",
      "lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 65.3, Train Acc: 70.058\n",
      "lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 67.6, Train Acc: 70.336\n",
      "lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 66.6, Train Acc: 70.629\n",
      "lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 67.3, Train Acc: 71.341\n",
      "lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 66.3, Train Acc: 71.705\n",
      "lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 66.7, Train Acc: 72.194\n",
      "lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 67.2, Train Acc: 71.993\n",
      "lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 67.5, Train Acc: 72.329\n",
      "lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 66.9, Train Acc: 72.711\n",
      "lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 69.0, Train Acc: 72.937\n",
      "lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 68.4, Train Acc: 73.659\n",
      "lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 67.8, Train Acc: 73.646\n",
      "lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 68.0, Train Acc: 74.445\n"
     ]
    }
   ],
   "source": [
    "CNN_model = CNN(emb_size=300, hidden_size=400, num_layers=2, num_classes=3, vocab_size=len(words_ft), kernel = 3)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 8 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "train_loss_vals_CNN = []\n",
    "train_acc_vals_CNN = []\n",
    "val_loss_vals_CNN = []\n",
    "val_acc_vals_CNN = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1,data2,lengths1,lengths2,labels,ordering1,ordering2) in enumerate(train_loader):\n",
    "        CNN_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = CNN_model(data1.to(device),data2.to(device),lengths1.to(device),lengths2.to(device),\\\n",
    "                        ordering1,ordering2)\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"lossing\")\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model(val_loader, CNN_model)\n",
    "            train_acc, train_loss = test_model(train_loader, CNN_model)\n",
    "            train_loss_vals_CNN.append(train_loss)\n",
    "            train_acc_vals_CNN.append(train_acc)\n",
    "            val_loss_vals_CNN.append(val_loss)\n",
    "            val_acc_vals_CNN.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc, train_acc ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021703"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params(CNN_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.6"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(val_loader, CNN_model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading mnli data\n",
    "mnli_val = pd.read_csv('mnli_val.tsv', sep='\\t') \n",
    "mnli_val['sentence1'] = mnli_val['sentence1'].str.split()\n",
    "mnli_val['sentence2'] = mnli_val['sentence2'].str.split()\n",
    "\n",
    "\n",
    "mnli_val['label'].replace('neutral',0, inplace=True)\n",
    "mnli_val['label'].replace('entailment',1, inplace = True)\n",
    "mnli_val['label'].replace('contradiction',2, inplace = True)\n",
    "\n",
    "#data_tup_train = zip(mnli_train.sentence1,mnli_train.sentence2,mnli_train.label,)\n",
    "data_tup_val_mnli = zip(mnli_val.sentence1,mnli_val.sentence2,mnli_val.label,mnli_val.genre)\n",
    "genres = np.unique(mnli_val.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_sen_length = 82\n",
    "\n",
    "class VocabDataset_mnli(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, char2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1,self.sent2,self.target_list,self.genre = zip(*data_tuple)\n",
    "        assert (len(self.sent1) == len(self.target_list))\n",
    "        assert (len(self.sent2) == len(self.target_list))\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        char_idx_1 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.sent1[key][:Max_sen_length]]\n",
    "        char_idx_2 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.sent2[key][:Max_sen_length]]\n",
    "        label = self.target_list[key]\n",
    "        genres = self.genre[key]\n",
    "        return [char_idx_1, char_idx_2,len(char_idx_1),len(char_idx_2),label,genres]\n",
    "\n",
    "def vocab_collate_func_mnli(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    length_list_2 = []\n",
    "    genre_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list.append(datum[2])\n",
    "        length_list_2.append(datum[3])\n",
    "        genre_list.append(datum[5])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,Max_sen_length-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,Max_sen_length-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec1)\n",
    "        data_list_2.append(padded_vec2)\n",
    "        \n",
    "\n",
    "    ordering1 = np.linspace(0, len(data_list_1),len(data_list_1), endpoint = False)\n",
    "    ordering2 = np.linspace(0, len(data_list_2),len(data_list_2), endpoint = False)\n",
    "      \n",
    "    ind_dec_order = np.argsort(length_list)[::-1]\n",
    "    ind_dec_order_2 = np.argsort(length_list_2)[::-1]\n",
    "    \n",
    "    ordering1 = ordering1[ind_dec_order]\n",
    "    ordering2 = ordering2[ind_dec_order_2]\n",
    "        \n",
    "    data_list_1 = np.array(data_list_1)[ind_dec_order]\n",
    "    data_list_2 = np.array(data_list_2)[ind_dec_order_2]\n",
    "                \n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    length_list_2 = np.array(length_list_2)[ind_dec_order_2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask1 = np.array(data_list_1)\n",
    "    mask2 = np.array(data_list_2)\n",
    "    \n",
    "    mask1[mask1 > 0] = 1\n",
    "    mask2[mask2 > 0] = 1\n",
    "    \n",
    "    \n",
    "    ###need to mask padded values###\n",
    "    \n",
    "                \n",
    "    #label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(data_list_1)), torch.from_numpy(np.array(data_list_2)), \\\n",
    "            torch.LongTensor(length_list),torch.LongTensor(length_list_2), \\\n",
    "            torch.LongTensor(label_list), ordering1, ordering2, genre_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_mnli = VocabDataset_mnli(data_tup_val_mnli, words_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_mnli = torch.utils.data.DataLoader(dataset=val_dataset_mnli,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func_mnli,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_mnli(loader, model, genre):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data1,data2,lengths1,lengths2,labels,ordering1,ordering2,genre_true in loader:\n",
    "            if genre != genre_true[0]:\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                outputs = model(data1.to(device),data2.to(device),lengths1,lengths2,\\\n",
    "                            ordering1,ordering2)\n",
    "                predicted = outputs.max(1, keepdim=True)[1]\n",
    "                loss = criterion(outputs, labels.to(device))\n",
    "                total_loss += loss/labels.size(0)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "                \n",
    "    return (100 * correct / total), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_performance = []\n",
    "cnn_performance = []\n",
    "for x in genres:\n",
    "    rnn_performance.append(test_model_mnli(val_loader_mnli, RNN_model, x)[0])\n",
    "    cnn_performance.append(test_model_mnli(val_loader_mnli, CNN_model, x)[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for x in val_loader_mnli:\n",
    "    print(x[4].size(0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.71859296482412,\n",
       " 44.881889763779526,\n",
       " 43.41317365269461,\n",
       " 47.16417910447761,\n",
       " 42.66802443991853]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.62311557788945,\n",
       " 44.09448818897638,\n",
       " 43.31337325349301,\n",
       " 45.27363184079602,\n",
       " 42.76985743380855]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "{} &        0 &           1 &        2 &          3 &        4 \\\\\n",
      "\\midrule\n",
      "Genre &  fiction &  government &    slate &  telephone &   travel \\\\\n",
      "RNN   &  43.7186 &     44.8819 &  43.4132 &    47.1642 &   42.668 \\\\\n",
      "CNN   &  44.6231 &     44.0945 &  43.3134 &    45.2736 &  42.7699 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnli_perf = pd.DataFrame({\"RNN\":rnn_performance, \"CNN\":cnn_performance,\"Genre\":genres })\n",
    "mnli_perf =mnli_perf[[\"Genre\", \"RNN\",\"CNN\"]].T\n",
    "print(mnli_perf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
