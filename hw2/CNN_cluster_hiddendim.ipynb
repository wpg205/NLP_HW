{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(42)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/wpg205'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/scratch/wpg205')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.append(PAD_IDX)\n",
    "    ordered_words_ft.append(UNK_IDX)\n",
    "    for t, line in enumerate(f):\n",
    "        i = t + 2 \n",
    "        if t >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "\n",
    "loaded_embeddings_ft_torch = torch.tensor(loaded_embeddings_ft, requires_grad = True)\n",
    "del loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "test\n",
      "is\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(ordered_words_ft[words_ft['the']])\n",
    "print(ordered_words_ft[words_ft['test']])\n",
    "print(ordered_words_ft[words_ft['is']])\n",
    "print(ordered_words_ft[words_ft['passed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = pd.read_csv('snli_train.tsv', sep='\\t') \n",
    "#snli_train = snli_train.iloc[:10000,:]\n",
    "snli_train['sentence1'] = snli_train['sentence1'].str.split()\n",
    "snli_train['sentence2'] = snli_train['sentence2'].str.split()\n",
    "\n",
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t') \n",
    "#snli_val = snli_val.iloc[:10000,:]\n",
    "snli_val['sentence1'] = snli_val['sentence1'].str.split()\n",
    "snli_val['sentence2'] = snli_val['sentence2'].str.split()\n",
    "\n",
    "snli_train['label'].replace('neutral',0, inplace=True)\n",
    "snli_train['label'].replace('entailment',1, inplace = True)\n",
    "snli_train['label'].replace('contradiction',2, inplace = True)\n",
    "\n",
    "snli_val['label'].replace('neutral',0, inplace=True)\n",
    "snli_val['label'].replace('entailment',1, inplace = True)\n",
    "snli_val['label'].replace('contradiction',2, inplace = True)\n",
    "\n",
    "data_tup_train = zip(snli_train.sentence1,snli_train.sentence2,snli_train.label)\n",
    "data_tup_val = zip(snli_val.sentence1,snli_val.sentence2,snli_val.label)\n",
    "\n",
    "del snli_train\n",
    "del snli_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_sen_length = 82\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, char2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1,self.sent2,self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.sent1) == len(self.target_list))\n",
    "        assert (len(self.sent2) == len(self.target_list))\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        char_idx_1 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.sent1[key][:Max_sen_length]]\n",
    "        char_idx_2 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.sent2[key][:Max_sen_length]]\n",
    "        label = self.target_list[key]\n",
    "        return [char_idx_1, char_idx_2,len(char_idx_1),len(char_idx_2),label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    length_list_2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list.append(datum[2])\n",
    "        length_list_2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,Max_sen_length-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,Max_sen_length-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec1)\n",
    "        data_list_2.append(padded_vec2)\n",
    "        \n",
    "\n",
    "    ordering1 = np.linspace(0, len(data_list_1),len(data_list_1), endpoint = False)\n",
    "    ordering2 = np.linspace(0, len(data_list_2),len(data_list_2), endpoint = False)\n",
    "      \n",
    "    ind_dec_order = np.argsort(length_list)[::-1]\n",
    "    ind_dec_order_2 = np.argsort(length_list_2)[::-1]\n",
    "    \n",
    "    ordering1 = ordering1[ind_dec_order]\n",
    "    ordering2 = ordering2[ind_dec_order_2]\n",
    "        \n",
    "    data_list_1 = np.array(data_list_1)[ind_dec_order]\n",
    "    data_list_2 = np.array(data_list_2)[ind_dec_order_2]\n",
    "                \n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    length_list_2 = np.array(length_list_2)[ind_dec_order_2]\n",
    "    \n",
    "    \n",
    "    mask1 = np.array(data_list_1)\n",
    "    mask2 = np.array(data_list_2)\n",
    "    \n",
    "    mask1[mask1 > 0] = 1\n",
    "    mask2[mask2 > 0] = 1\n",
    "    \n",
    "    \n",
    "    ###need to mask padded values###\n",
    "    \n",
    "                \n",
    "    #label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(data_list_1)), torch.from_numpy(np.array(data_list_2)), \\\n",
    "            torch.LongTensor(length_list),torch.LongTensor(length_list_2), \\\n",
    "            torch.LongTensor(label_list), ordering1, ordering2, \\\n",
    "            torch.from_numpy(mask1).to(device), torch.from_numpy(mask2).to(device)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VocabDataset(data_tup_train, words_ft)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(data_tup_val, words_ft)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "del train_dataset\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size,kernel = 3):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding = nn.Embedding.from_pretrained(loaded_embeddings_ft_torch, freeze = False).to(device)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel, padding=1).to(device)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel, padding=1).to(device)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size*2, 200).to(device)\n",
    "        self.linear2 = nn.Linear(200, 100).to(device)\n",
    "        self.linear = nn.Linear(100, num_classes).to(device)\n",
    "\n",
    "    def forward(self, x1,x2,lengths1,lengths2, ordering1, ordering2, mask1, mask2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "        \"\"\"\n",
    "        mask1_size1 = mask1.size(0)\n",
    "        mask1_size2 = mask1.size(1)\n",
    "        \n",
    "        mask2_size1 = mask2.size(0)\n",
    "        mask2_size2 = mask2.size(1)\n",
    "        \n",
    "        mask1.unsqueeze_(-1)\n",
    "        mask1 = mask1.expand(mask1_size1,mask1_size2,200).to(torch.float).to(device)\n",
    "        \n",
    "        mask2.unsqueeze_(-1)\n",
    "        mask2 = mask2.expand(mask2_size1,mask2_size2,200).to(torch.float).to(device)\n",
    "        \"\"\"\n",
    "        \n",
    "        m1 = torch.zeros([x1.size(0), x1.size(1)]).to(device)\n",
    "        m2 = torch.zeros([x2.size(0), x2.size(1)]).to(device)\n",
    "        \n",
    "        m1[x1 == 1] = 1\n",
    "        m2[x2 == 1] = 1\n",
    "\n",
    "        embed1 = self.embedding(x1).to(device)\n",
    "        embed2 = self.embedding(x2).to(device)\n",
    "        \n",
    "        m1 = m1.unsqueeze(-1).expand_as(embed1)\n",
    "        m2 = m2.unsqueeze(-1).expand_as(embed2)\n",
    "        \n",
    "        embed1 = embed1.to(torch.float)\n",
    "        embed2 = embed2.to(torch.float)\n",
    "        \n",
    "        m1 = m1.to(torch.float)\n",
    "        m2 = m2.to(torch.float)\n",
    "\n",
    "        embed1 = m1 * embed1+ (1-m1) * embed1.clone().detach()\n",
    "        embed2 = m2 * embed2+ (1-m2) * embed2.clone().detach()        \n",
    "        \n",
    "        hidden = self.conv1(embed1.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        drop1  = nn.Dropout()\n",
    "        hidden = drop1(hidden)\n",
    "\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, -1, hidden.size(-1))\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        hidden = drop1(hidden)\n",
    "        \n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, -1, hidden.size(-1))\n",
    "        #hidden = hidden*mask1.to(device)\n",
    "        hidden = hidden.max(dim = 1)[0]\n",
    "        #hidden = torch.sum(hidden, dim=1)\n",
    "        #print(hidden.size())\n",
    "        \n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        hidden2 = drop1(hidden2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, -1, hidden2.size(-1))\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        \n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2).to(device)\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        hidden2 = drop1(hidden2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, -1, hidden2.size(-1))\n",
    "        #hidden2 = hidden2*mask2.to(device)\n",
    "        hidden2 = hidden2.max(dim = 1)[0]\n",
    "        #hidden2 = torch.sum(hidden2, dim=1)\n",
    "        \n",
    "        \n",
    "        reverse1 = np.argsort(ordering1)\n",
    "        reverse2 = np.argsort(ordering2)\n",
    "    \n",
    "        cnn_out1 = hidden[reverse1,:]\n",
    "        cnn_out2 = hidden2[reverse2,:]\n",
    "        #print(cnn_out1.size())\n",
    "        #cnn_out1 = cnn_out1[:,-1]\n",
    "        #cnn_out2 = cnn_out2[:,-1]\n",
    "        #print(cnn_out1.size())\n",
    "        \n",
    "        cnn_out = torch.cat((cnn_out1,cnn_out2), 1)\n",
    "        \n",
    "        cnn_out = F.relu(self.linear1(cnn_out))\n",
    "        cnn_out = F.relu(self.linear2(cnn_out))\n",
    "        logits = self.linear(cnn_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data1,data2,lengths1,lengths2,labels,ordering1,ordering2, mask1, mask2 in loader:\n",
    "            outputs = F.softmax(model(data1.to(device),data2.to(device), lengths1.to(device),\\\n",
    "                                      lengths2.to(device),ordering1,ordering2,mask1,mask2), dim=0)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            total_loss += loss/labels.to(device).size(0)\n",
    "            total += labels.to(device).size(0)\n",
    "            correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_hidd(hidden_var):\n",
    "    print(hidden_var)\n",
    "    model = CNN(emb_size=300, hidden_size=hidden_var, num_layers=2, num_classes=3, vocab_size=len(words_ft), kernel = 3)\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 8 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    train_loss_vals = []\n",
    "    train_acc_vals = []\n",
    "    val_loss_vals = []\n",
    "    val_acc_vals = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data1,data2,lengths1,lengths2,labels,ordering1,ordering2,mask1,mask2) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data1.to(device),data2.to(device),lengths1.to(device),lengths2.to(device),\\\n",
    "                            ordering1,ordering2,mask1, mask2)\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                print(\"lossing\")\n",
    "                # validate\n",
    "                val_acc, val_loss = test_model(val_loader, model)\n",
    "                train_acc, train_loss = test_model(train_loader, model)\n",
    "                train_loss_vals.append(train_loss)\n",
    "                train_acc_vals.append(train_acc)\n",
    "                val_loss_vals.append(val_loss)\n",
    "                val_acc_vals.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, total_step, val_acc, train_acc ))\n",
    "        \n",
    "    return train_loss_vals, train_acc_vals, val_loss_vals, val_acc_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 52.0, Train Acc: 53.623\n",
      "lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 57.7, Train Acc: 58.933\n",
      "lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 58.1, Train Acc: 60.107\n",
      "lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 60.5, Train Acc: 60.612\n",
      "lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 60.3, Train Acc: 61.108\n",
      "lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 58.2, Train Acc: 60.989\n",
      "lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 61.6, Train Acc: 61.888\n",
      "lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 62.0, Train Acc: 62.037\n",
      "lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 61.1, Train Acc: 61.437\n",
      "lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 62.9, Train Acc: 62.67\n",
      "lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 60.4, Train Acc: 61.761\n",
      "lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 61.0, Train Acc: 62.851\n",
      "lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 60.2, Train Acc: 62.73\n",
      "lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 63.3, Train Acc: 63.787\n",
      "lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 60.5, Train Acc: 64.022\n",
      "lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 61.4, Train Acc: 63.669\n",
      "lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 59.2, Train Acc: 63.284\n",
      "lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 61.5, Train Acc: 64.304\n",
      "lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 60.6, Train Acc: 64.548\n",
      "lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 61.0, Train Acc: 65.147\n",
      "lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 61.6, Train Acc: 64.314\n",
      "lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 63.5, Train Acc: 65.08\n",
      "lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 61.3, Train Acc: 64.813\n",
      "lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 62.6, Train Acc: 64.384\n",
      "200\n",
      "lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 54.8, Train Acc: 57.241\n",
      "lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 59.0, Train Acc: 60.326\n",
      "lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 61.4, Train Acc: 61.085\n",
      "lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 58.3, Train Acc: 61.677\n",
      "lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 61.4, Train Acc: 62.133\n",
      "lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 61.1, Train Acc: 62.094\n",
      "lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 61.3, Train Acc: 62.999\n",
      "lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 61.0, Train Acc: 62.304\n",
      "lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 61.9, Train Acc: 63.331\n",
      "lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 63.4, Train Acc: 63.481\n",
      "lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 62.7, Train Acc: 64.149\n",
      "lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 61.0, Train Acc: 63.991\n",
      "lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 62.4, Train Acc: 63.931\n",
      "lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 62.3, Train Acc: 64.604\n",
      "lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 63.3, Train Acc: 65.219\n",
      "lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 62.6, Train Acc: 64.51\n",
      "lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 60.6, Train Acc: 65.085\n",
      "lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 63.4, Train Acc: 65.327\n",
      "lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 61.8, Train Acc: 65.453\n",
      "lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 60.5, Train Acc: 65.191\n",
      "lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 63.1, Train Acc: 65.741\n",
      "lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 62.4, Train Acc: 65.756\n",
      "lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 62.2, Train Acc: 66.323\n",
      "lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 63.5, Train Acc: 65.944\n",
      "250\n",
      "lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 51.9, Train Acc: 52.413\n",
      "lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 58.8, Train Acc: 59.618\n",
      "lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 59.9, Train Acc: 61.528\n",
      "lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 60.1, Train Acc: 62.425\n",
      "lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 62.5, Train Acc: 63.154\n",
      "lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 61.6, Train Acc: 63.222\n",
      "lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 63.8, Train Acc: 63.888\n",
      "lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 62.0, Train Acc: 64.456\n",
      "lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 63.8, Train Acc: 65.096\n",
      "lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 64.3, Train Acc: 65.253\n",
      "lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 65.7, Train Acc: 65.592\n",
      "lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 65.1, Train Acc: 66.474\n",
      "lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 63.8, Train Acc: 66.099\n",
      "lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 63.2, Train Acc: 66.476\n",
      "lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 63.0, Train Acc: 67.267\n",
      "lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 63.0, Train Acc: 66.886\n",
      "lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 64.2, Train Acc: 67.324\n",
      "lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 64.5, Train Acc: 67.398\n",
      "lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 64.5, Train Acc: 67.267\n",
      "lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 64.9, Train Acc: 68.212\n",
      "lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 63.3, Train Acc: 68.793\n",
      "lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 65.0, Train Acc: 68.542\n",
      "lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 65.3, Train Acc: 68.697\n",
      "lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 64.8, Train Acc: 69.388\n",
      "300\n",
      "lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 57.5, Train Acc: 56.824\n",
      "lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 58.6, Train Acc: 60.463\n",
      "lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 58.2, Train Acc: 61.561\n",
      "lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 60.9, Train Acc: 62.11\n",
      "lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 62.5, Train Acc: 62.096\n",
      "lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 62.0, Train Acc: 63.328\n",
      "lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 61.4, Train Acc: 63.045\n",
      "lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 61.9, Train Acc: 63.746\n",
      "lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 64.7, Train Acc: 64.529\n",
      "lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 63.1, Train Acc: 65.859\n",
      "lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 62.0, Train Acc: 65.676\n",
      "lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 64.2, Train Acc: 66.125\n",
      "lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 62.6, Train Acc: 65.605\n",
      "lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 62.1, Train Acc: 65.935\n",
      "lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 62.9, Train Acc: 67.415\n",
      "lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 61.7, Train Acc: 66.863\n",
      "lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 66.0, Train Acc: 67.793\n",
      "lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 62.3, Train Acc: 67.563\n",
      "lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 62.7, Train Acc: 68.062\n",
      "lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 64.6, Train Acc: 67.952\n",
      "lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 64.6, Train Acc: 68.596\n",
      "lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 64.3, Train Acc: 68.024\n",
      "lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 64.4, Train Acc: 69.062\n",
      "lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 65.5, Train Acc: 68.098\n",
      "400\n",
      "lossing\n",
      "Epoch: [1/8], Step: [1001/3125], Validation Acc: 56.1, Train Acc: 57.5\n",
      "lossing\n",
      "Epoch: [1/8], Step: [2001/3125], Validation Acc: 60.3, Train Acc: 61.214\n",
      "lossing\n",
      "Epoch: [1/8], Step: [3001/3125], Validation Acc: 61.7, Train Acc: 62.495\n",
      "lossing\n",
      "Epoch: [2/8], Step: [1001/3125], Validation Acc: 61.5, Train Acc: 63.08\n",
      "lossing\n",
      "Epoch: [2/8], Step: [2001/3125], Validation Acc: 61.3, Train Acc: 62.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossing\n",
      "Epoch: [2/8], Step: [3001/3125], Validation Acc: 62.1, Train Acc: 63.925\n",
      "lossing\n",
      "Epoch: [3/8], Step: [1001/3125], Validation Acc: 59.7, Train Acc: 64.559\n",
      "lossing\n",
      "Epoch: [3/8], Step: [2001/3125], Validation Acc: 63.6, Train Acc: 64.916\n",
      "lossing\n",
      "Epoch: [3/8], Step: [3001/3125], Validation Acc: 62.7, Train Acc: 65.054\n",
      "lossing\n",
      "Epoch: [4/8], Step: [1001/3125], Validation Acc: 64.3, Train Acc: 65.608\n",
      "lossing\n",
      "Epoch: [4/8], Step: [2001/3125], Validation Acc: 62.9, Train Acc: 66.02\n",
      "lossing\n",
      "Epoch: [4/8], Step: [3001/3125], Validation Acc: 62.1, Train Acc: 65.966\n",
      "lossing\n",
      "Epoch: [5/8], Step: [1001/3125], Validation Acc: 63.8, Train Acc: 66.553\n",
      "lossing\n",
      "Epoch: [5/8], Step: [2001/3125], Validation Acc: 63.0, Train Acc: 66.201\n",
      "lossing\n",
      "Epoch: [5/8], Step: [3001/3125], Validation Acc: 66.1, Train Acc: 67.937\n",
      "lossing\n",
      "Epoch: [6/8], Step: [1001/3125], Validation Acc: 64.5, Train Acc: 68.059\n",
      "lossing\n",
      "Epoch: [6/8], Step: [2001/3125], Validation Acc: 63.5, Train Acc: 68.138\n",
      "lossing\n",
      "Epoch: [6/8], Step: [3001/3125], Validation Acc: 63.9, Train Acc: 68.092\n",
      "lossing\n",
      "Epoch: [7/8], Step: [1001/3125], Validation Acc: 63.8, Train Acc: 68.419\n",
      "lossing\n",
      "Epoch: [7/8], Step: [2001/3125], Validation Acc: 63.7, Train Acc: 68.351\n",
      "lossing\n",
      "Epoch: [7/8], Step: [3001/3125], Validation Acc: 65.5, Train Acc: 69.235\n",
      "lossing\n",
      "Epoch: [8/8], Step: [1001/3125], Validation Acc: 64.5, Train Acc: 69.452\n",
      "lossing\n",
      "Epoch: [8/8], Step: [2001/3125], Validation Acc: 64.4, Train Acc: 69.06\n",
      "lossing\n",
      "Epoch: [8/8], Step: [3001/3125], Validation Acc: 64.2, Train Acc: 69.927\n"
     ]
    }
   ],
   "source": [
    "hidden_dim_list = [100,200,250,300,400]\n",
    "\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "training_accs = []\n",
    "val_accs = []\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "for x in hidden_dim_list:\n",
    "    a,b,c,d = CNN_hidd(x)\n",
    "    training_losses.append(a)\n",
    "    training_accs.append(b)\n",
    "    val_losses.append(c)\n",
    "    val_accs.append(d)\n",
    "    pkl.dump(training_losses, open(\"training_losses_cnn_hid.p\", \"wb\"))\n",
    "    pkl.dump(val_losses, open(\"val_losses_cnn_hid.p\", \"wb\"))\n",
    "    pkl.dump(training_accs, open(\"training_accs_cnn_hid.p\", \"wb\"))\n",
    "    pkl.dump(val_accs, open(\"val_accs_cnn_hid.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_accs = []\n",
    "for x in val_accs:\n",
    "    max_val_accs.append(max(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &      0 &      1 &      2 &      3 &      4 \\\\\n",
      "\\midrule\n",
      "Hidden Sizes   &  100.0 &  200.0 &  250.0 &  300.0 &  400.0 \\\\\n",
      "Validation\\_acc &   63.5 &   63.5 &   65.7 &   66.0 &   66.1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({\"Hidden Sizes\":hidden_dim_list,\"Validation_acc\":max_val_accs}).T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_params(model):\n",
    "    param_total = 0\n",
    "    for x in model.parameters():\n",
    "        if x.requires_grad:\n",
    "            store = 1\n",
    "            for y in range(len(x.size())):\n",
    "                store = store*x.size(y)\n",
    "            param_total += store\n",
    "    \n",
    "    #(these are untrained embeddings)\n",
    "    total_embeddings = 50001*300\n",
    "    param_total -= total_embeddings\n",
    "    return param_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_list = [100,200,250,300,400]\n",
    "params_list = []\n",
    "for x in hidden_dim_list:\n",
    "    model = CNN(emb_size=300, hidden_size=x, num_layers=2, num_classes=3, vocab_size=len(words_ft), kernel = 3)\n",
    "    params_list.append(total_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[181103, 401303, 533903, 681503, 1021703]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
