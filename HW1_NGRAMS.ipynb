{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "import tqdm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train negative reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/train/neg')\n",
    "\n",
    "neg_rev = []\n",
    "for x in os.listdir():\n",
    "    neg_rev.append(open(x,'r').read())\n",
    "\n",
    "\n",
    "#loading train positive reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/train/pos')\n",
    "\n",
    "pos_rev = []\n",
    "for x in os.listdir():\n",
    "    pos_rev.append(open(x,'r').read())\n",
    "\n",
    "all_rev = neg_rev\n",
    "all_rev.extend(pos_rev)\n",
    "train_data = pd.DataFrame({\"reviews\":all_rev})\n",
    "train_data['positive'] = 0\n",
    "train_data.iloc[12500:,1] = 1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data['reviews'], train_data['positive'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test negative reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/test/neg')\n",
    "\n",
    "neg_rev = []\n",
    "for x in os.listdir():\n",
    "    neg_rev.append(open(x,'r').read())\n",
    "\n",
    "\n",
    "#loading test positive reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/test/pos')\n",
    "\n",
    "pos_rev = []\n",
    "for x in os.listdir():\n",
    "    pos_rev.append(open(x,'r').read())\n",
    "\n",
    "all_rev = neg_rev\n",
    "all_rev.extend(pos_rev)\n",
    "test_data = pd.DataFrame({\"reviews\":all_rev})\n",
    "test_data['positive'] = 0\n",
    "test_data.iloc[12500:,1] = 1\n",
    "X_test, y_test = test_data.iloc[:,0], test_data.iloc[:,1]\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pos_rev, neg_rev, all_rev, train_data, test_data\n",
    "del X_train, X_test, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "train_data_tokens_2 = pkl.load(open(\"train_data_tokens_2.p\", \"rb\"))\n",
    "all_train_tokens_2 = pkl.load(open(\"all_train_tokens_2.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_2 = pkl.load(open(\"val_data_tokens_2.p\", \"rb\"))\n",
    "test_data_tokens_2 = pkl.load(open(\"test_data_tokens_2.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, y_val)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def Model_train(emb_dim, model, learning_rate, \\\n",
    "                num_epochs, criterion, optimizer, \\\n",
    "                train_loader):\n",
    "    \n",
    "\n",
    "\n",
    "    loss_vals = []\n",
    "    acc_est = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss_vals.append(loss/labels.size(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                acc_est.append(val_acc)\n",
    "                #loss_vals.append(test_model_LOSS(train_loader,model))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    \n",
    "    return loss_vals, acc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/625], Validation Acc: 53.96\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 54.02\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 62.76\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 66.62\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 65.96\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 69.68\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 74.4\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 76.08\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 77.24\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 79.26\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 79.88\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 79.62\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 81.2\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 82.06\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 82.58\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 83.14\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 83.14\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 83.5\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 83.72\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 83.86\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 84.24\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 84.36\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 84.44\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 84.5\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 84.74\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 85.1\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 85.38\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 85.34\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 85.18\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 85.64\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 53.38\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 63.18\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 67.5\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 70.88\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 72.9\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 73.36\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 76.24\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 78.84\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 81.12\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 80.6\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 82.52\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 82.98\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 83.38\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 83.74\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 83.84\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 84.08\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 84.58\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 84.7\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 84.34\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 85.0\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 85.02\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 85.34\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 85.54\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 85.74\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 85.62\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 85.58\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 85.84\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 85.78\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.02\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.28\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 56.16\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 67.64\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 65.94\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 73.92\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 77.12\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 79.08\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 81.46\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 82.72\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 82.34\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 82.44\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.12\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 84.52\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 83.54\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 85.2\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 85.04\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 84.36\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 85.02\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 85.5\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 85.8\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 84.62\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 85.6\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 86.1\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 85.94\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 85.72\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 86.18\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 86.08\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 86.02\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.26\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.18\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.26\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 59.98\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 69.78\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 68.08\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 76.22\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 76.12\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 81.88\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 83.46\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 82.26\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 80.82\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 83.46\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 85.28\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 83.64\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 84.9\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 85.98\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 84.42\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 86.0\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 85.96\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 86.1\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 86.32\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 86.16\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 86.26\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 86.16\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 86.4\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 85.18\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 86.26\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 85.62\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 86.18\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.06\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.56\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 5# number epoch to train\n",
    "learning_rates = .001\n",
    "embedd_dim = [100,200,500,1000]\n",
    "\n",
    "loss_performance = []\n",
    "acc_performance = []\n",
    "\n",
    "for o in embedd_dim:\n",
    "    \n",
    "    emb_dim = o\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "    \n",
    "    loss_vals, acc_est = Model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                     num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                     train_loader = train_loader)\n",
    "    \n",
    "    loss_performance.append(loss_vals)\n",
    "    acc_performance.append(test_model(val_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85.3, 86.06, 86.22, 86.12]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/625], Validation Acc: 51.42\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 58.44\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 57.76\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 63.82\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 67.5\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 69.7\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 73.08\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 74.34\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 77.04\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 78.4\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 79.8\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 80.72\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 81.78\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 82.44\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 82.7\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 83.2\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 82.66\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 83.54\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 83.22\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 83.94\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 84.08\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 84.54\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 84.18\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 84.72\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 85.2\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 84.98\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 84.98\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 85.16\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 85.46\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 85.32\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 59.48\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 62.02\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 67.28\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 69.22\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 72.6\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 72.8\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 78.72\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 79.74\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 81.6\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 82.22\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 82.36\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 82.58\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 83.12\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 83.82\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 83.7\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 84.24\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 84.2\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 84.78\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 84.48\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 84.96\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 85.2\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 85.22\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 85.46\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 84.96\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 85.58\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 85.8\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 84.96\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.1\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.24\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 50.32\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 67.48\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 71.44\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 69.52\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 77.1\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 79.28\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 80.36\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 82.16\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 83.56\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 82.96\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.08\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 84.72\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 84.82\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 85.06\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 85.2\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 85.54\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 85.48\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 85.84\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 86.08\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 84.42\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 85.44\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 86.06\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 86.48\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 85.9\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 86.02\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 86.32\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 86.62\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.68\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.76\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.22\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 66.16\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 71.12\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 75.72\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 79.12\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 79.72\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 82.44\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 82.94\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 83.3\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 83.46\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 84.54\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 85.08\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 84.68\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 85.46\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 84.66\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 85.52\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 85.56\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 85.46\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 86.02\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 86.06\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 85.82\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 85.52\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 85.4\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 86.22\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 86.04\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 86.1\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 86.1\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 86.06\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.16\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.02\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.02\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "train_data_tokens_3 = pkl.load(open(\"train_data_tokens_3.p\", \"rb\"))\n",
    "all_train_tokens_3 = pkl.load(open(\"all_train_tokens_3.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_3 = pkl.load(open(\"val_data_tokens_3.p\", \"rb\"))\n",
    "test_data_tokens_3 = pkl.load(open(\"test_data_tokens_3.p\", \"rb\"))\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_3)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_3)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_3)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_3)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, y_val)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "num_epochs = 5# number epoch to train\n",
    "learning_rates = .001\n",
    "embedd_dim = [100,200,500,1000]\n",
    "\n",
    "loss_performance1 = []\n",
    "acc_performance1 = []\n",
    "\n",
    "for o in embedd_dim:\n",
    "    \n",
    "    emb_dim = o\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "    \n",
    "    loss_vals, acc_est = Model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                     num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                     train_loader = train_loader)\n",
    "    \n",
    "    loss_performance1.append(loss_vals)\n",
    "    acc_performance1.append(test_model(val_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/625], Validation Acc: 49.84\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 59.1\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 61.48\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 63.98\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 67.3\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 71.14\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 71.52\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 74.52\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 76.18\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 78.02\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 79.24\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 80.44\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 81.02\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 81.86\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 82.18\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 82.78\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 83.12\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 83.28\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 83.56\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 83.62\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 83.66\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 84.06\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 84.3\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 84.6\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 84.38\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 84.78\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 84.86\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 85.02\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 85.36\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 85.26\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 57.22\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 67.06\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 67.22\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 71.12\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 71.4\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 75.18\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 75.68\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 78.14\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 81.28\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 81.5\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 82.28\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 82.74\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 83.22\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 83.28\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 83.86\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 83.16\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 84.52\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 83.94\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 85.32\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 84.5\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 85.32\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 85.1\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 85.2\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 84.96\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 85.54\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 86.02\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 85.7\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.18\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.14\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 85.76\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 58.52\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 64.9\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 72.6\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 70.08\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 78.16\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 76.96\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 80.04\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 82.2\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 82.88\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 83.2\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.04\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 84.54\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 84.96\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 85.28\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 85.24\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 84.52\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 85.94\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 85.48\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 86.16\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 85.96\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 86.12\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 86.24\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 86.34\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 86.2\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 86.08\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 86.32\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 86.46\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.48\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.06\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 86.3\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 64.28\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 68.26\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 74.98\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 75.92\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 81.2\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 82.38\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 83.3\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 83.58\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 83.46\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 84.32\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.28\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 85.32\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 85.84\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 85.84\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 85.08\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 85.84\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 84.86\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 85.82\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 86.1\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 86.42\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 86.12\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 85.88\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 86.28\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 86.18\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 85.46\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 86.38\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 86.16\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 86.18\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 86.28\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 85.18\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "train_data_tokens_4 = pkl.load(open(\"train_data_tokens_4.p\", \"rb\"))\n",
    "all_train_tokens_4 = pkl.load(open(\"all_train_tokens_4.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_4 = pkl.load(open(\"val_data_tokens_4.p\", \"rb\"))\n",
    "test_data_tokens_4 = pkl.load(open(\"test_data_tokens_4.p\", \"rb\"))\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_4)\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_4)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_4)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_4)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, y_val)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "num_epochs = 5# number epoch to train\n",
    "learning_rates = .001\n",
    "embedd_dim = [100,200,500,1000]\n",
    "\n",
    "loss_performance2 = []\n",
    "acc_performance2 = []\n",
    "\n",
    "for o in embedd_dim:\n",
    "    \n",
    "    emb_dim = o\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "    \n",
    "    loss_vals, acc_est = Model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                     num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                     train_loader = train_loader)\n",
    "    \n",
    "    loss_performance2.append(loss_vals)\n",
    "    acc_performance2.append(test_model(val_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_table = np.vstack((acc_performance, acc_performance1, acc_performance2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &      0 &      1 &      2 &      3 \\\\\n",
      "\\midrule\n",
      "0 &  85.30 &  86.06 &  86.22 &  86.12 \\\\\n",
      "1 &  85.94 &  86.22 &  86.56 &  86.02 \\\\\n",
      "2 &  85.32 &  85.40 &  85.96 &  85.98 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(perf_table).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
