{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tqdm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "path = '/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train negative reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/train/neg')\n",
    "\n",
    "neg_rev = []\n",
    "for x in os.listdir():\n",
    "    neg_rev.append(open(x,'r').read())\n",
    "\n",
    "\n",
    "#loading train positive reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/train/pos')\n",
    "\n",
    "pos_rev = []\n",
    "for x in os.listdir():\n",
    "    pos_rev.append(open(x,'r').read())\n",
    "\n",
    "all_rev = neg_rev\n",
    "all_rev.extend(pos_rev)\n",
    "train_data = pd.DataFrame({\"reviews\":all_rev})\n",
    "train_data['positive'] = 0\n",
    "train_data.iloc[12500:,1] = 1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data['reviews'], train_data['positive'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test negative reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/test/neg')\n",
    "\n",
    "neg_rev = []\n",
    "for x in os.listdir():\n",
    "    neg_rev.append(open(x,'r').read())\n",
    "\n",
    "\n",
    "#loading test positive reviews\n",
    "os.chdir('/Users/williamgodel/Google Drive/Grad School/Year Three/NLP/HW1/aclImdb/test/pos')\n",
    "\n",
    "pos_rev = []\n",
    "for x in os.listdir():\n",
    "    pos_rev.append(open(x,'r').read())\n",
    "\n",
    "all_rev = neg_rev\n",
    "all_rev.extend(pos_rev)\n",
    "test_data = pd.DataFrame({\"reviews\":all_rev})\n",
    "test_data['positive'] = 0\n",
    "test_data.iloc[12500:,1] = 1\n",
    "X_test, y_test = test_data.iloc[:,0], test_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "print (\"Train dataset size is {}\".format(len(X_train)))\n",
    "print (\"Val dataset size is {}\".format(len(X_val)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent, num_grams):\n",
    "    tokens = tokenizer(sent)\n",
    "    tokens_1 = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    ngram_lst = []\n",
    "    if num_grams > 1:\n",
    "        for t in range(2,num_grams+1):\n",
    "            ngram_lst += (nltk.ngrams(tokens_1,t))\n",
    "    all_tokens = tokens_1 + ngram_lst\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "def tokenize_dataset(dataset, num_grams):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, num_grams)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "N_grams = 1\n",
    "\n",
    "val_data_tokens, _ = tokenize_dataset(X_val,N_grams)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(X_test,N_grams)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(X_train,N_grams)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "N_grams = 2\n",
    "\n",
    "val_data_tokens_2, _ = tokenize_dataset(X_val,N_grams)\n",
    "pkl.dump(val_data_tokens_2, open(\"val_data_tokens_2.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_2, _ = tokenize_dataset(X_test,N_grams)\n",
    "pkl.dump(test_data_tokens_2, open(\"test_data_tokens_2.p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_2, all_train_tokens_2 = tokenize_dataset(X_train,N_grams)\n",
    "pkl.dump(train_data_tokens_2, open(\"train_data_tokens_2.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_2, open(\"all_train_tokens_2.p\", \"wb\"))\n",
    "\n",
    "N_grams = 3\n",
    "\n",
    "val_data_tokens_3, _ = tokenize_dataset(X_val,N_grams)\n",
    "pkl.dump(val_data_tokens_3, open(\"val_data_tokens_3.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_3, _ = tokenize_dataset(X_test,N_grams)\n",
    "pkl.dump(test_data_tokens_3, open(\"test_data_tokens_3.p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_3, all_train_tokens_3 = tokenize_dataset(X_train,N_grams)\n",
    "pkl.dump(train_data_tokens_3, open(\"train_data_tokens_3.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_3, open(\"all_train_tokens_3.p\", \"wb\"))\n",
    "\n",
    "N_grams = 4\n",
    "\n",
    "val_data_tokens_4, _ = tokenize_dataset(X_val,N_grams)\n",
    "pkl.dump(val_data_tokens_4, open(\"val_data_tokens_4.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_4, _ = tokenize_dataset(X_test,N_grams)\n",
    "pkl.dump(test_data_tokens_4, open(\"test_data_tokens_4.p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_4, all_train_tokens_4 = tokenize_dataset(X_train,N_grams)\n",
    "pkl.dump(train_data_tokens_4, open(\"train_data_tokens_4.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_4, open(\"all_train_tokens_4.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pos_rev, neg_rev, all_rev, train_data, test_data\n",
    "del X_train, X_test, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "#train_data_indices = token2index_dataset(train_data_tokens)\n",
    "#val_data_indices = token2index_dataset(val_data_tokens)\n",
    "#test_data_indices = token2index_dataset(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list, max_length):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        self.max_length = max_length\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:self.max_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general model function for testing hyper parameters\n",
    "\n",
    "\n",
    "def Model_train(emb_dim, model, learning_rate, \\\n",
    "                num_epochs, criterion, optimizer, \\\n",
    "                train_loader):\n",
    "    \n",
    "    loss_vals = []\n",
    "    acc_est = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss_vals.append(loss/labels.size(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                acc_est.append(val_acc)\n",
    "                #loss_vals.append(test_model_LOSS(train_loader,model))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    \n",
    "    return loss_vals, acc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check1\n",
      "check2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 56.12\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 62.2\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 67.52\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 69.96\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 72.44\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 72.92\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 75.42\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 76.34\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 78.22\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 78.94\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 79.56\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 79.5\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 80.6\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 80.74\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 80.5\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 81.88\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 81.52\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 81.8\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 81.82\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 82.32\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 82.32\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 82.32\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 82.64\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 82.78\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 82.88\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 82.98\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 83.02\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 82.78\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 82.68\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 82.86\n",
      "check3\n",
      "check1\n",
      "check2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 61.42\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 64.26\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 71.58\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 75.26\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 77.66\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 78.94\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 81.38\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 82.46\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 83.5\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 83.64\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.74\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 85.24\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 85.62\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 85.84\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 86.12\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 86.54\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 86.88\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 86.8\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 86.74\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 87.26\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 87.36\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 87.6\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 87.82\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 88.04\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 87.84\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 88.06\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 88.06\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 87.88\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 88.12\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 88.2\n",
      "check3\n",
      "check1\n",
      "check2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 58.82\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 61.26\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 69.76\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 75.3\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 78.18\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 79.68\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 80.24\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 83.2\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 84.36\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 85.0\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.94\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 85.32\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 86.24\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 86.44\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 86.62\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 87.34\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 87.3\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 86.98\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 87.8\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 88.16\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 87.8\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 88.2\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 88.3\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 88.58\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 88.52\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 88.8\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 88.68\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 88.7\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 88.72\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 88.98\n",
      "check3\n",
      "check1\n",
      "check2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 53.06\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 60.3\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 65.46\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 68.64\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 71.3\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 73.54\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 75.88\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 77.1\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 78.6\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 79.28\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 80.52\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 80.3\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 81.62\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 82.2\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 82.28\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 82.24\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 82.58\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 82.88\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 83.12\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 83.16\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 83.0\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 83.58\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 83.54\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 83.56\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 83.58\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 83.44\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 83.54\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 83.74\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 83.9\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 83.94\n",
      "check3\n",
      "check1\n",
      "check2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 61.36\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 67.48\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 70.28\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 74.68\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 77.22\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 79.48\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 81.96\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 82.94\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 83.72\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 84.02\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 84.9\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 85.68\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 86.08\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 86.58\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 86.9\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 87.16\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 87.26\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 87.48\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 87.82\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 88.1\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 88.14\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 88.34\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 88.46\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 88.52\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 88.36\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 88.52\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 88.64\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 89.04\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 89.18\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 88.9\n",
      "check3\n",
      "check1\n",
      "check2\n",
      "Epoch: [1/5], Step: [101/625], Validation Acc: 51.24\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 69.74\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 73.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [401/625], Validation Acc: 75.72\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 78.28\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 80.34\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 82.62\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 84.02\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 84.66\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 85.38\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 86.1\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 86.78\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 87.5\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 87.86\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 87.98\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 88.04\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 88.08\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 88.28\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 88.54\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 88.84\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 89.0\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 88.92\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 89.08\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 89.02\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 89.2\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 89.16\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 89.3\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 89.36\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 89.32\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 89.4\n",
      "check3\n"
     ]
    }
   ],
   "source": [
    "vocab_length  = [10000, 50000]\n",
    "max_sentence = [100,300,500]\n",
    "\n",
    "loss_performance2 = []\n",
    "acc_performance2 = []\n",
    "\n",
    "for x in vocab_length:\n",
    "    for z in max_sentence:\n",
    "    \n",
    "        max_vocab_size = x\n",
    "        MAX_SENTENCE_LENGTH = x\n",
    "        print('check1')\n",
    "        token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "\n",
    "        train_data_indices = token2index_dataset(train_data_tokens)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens)\n",
    "        test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "        BATCH_SIZE = 32\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, y_train,z)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=newsgroup_collate_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = NewsGroupDataset(val_data_indices, y_val,z)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 collate_fn=newsgroup_collate_func,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "        test_dataset = NewsGroupDataset(test_data_indices, y_test,z)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  collate_fn=newsgroup_collate_func,\n",
    "                                                  shuffle=False)\n",
    "        num_epochs = 5\n",
    "    \n",
    "        emb_dim = 200\n",
    "        \n",
    "        model = BagOfWords(len(id2token), emb_dim)\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "        print('check2')\n",
    "        loss_vals, acc_est = Model_train(emb_dim = emb_dim, model = model, learning_rate = .001, \\\n",
    "                                         num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                         train_loader = train_loader)\n",
    "        print('check3')\n",
    "        loss_performance2.append(loss_vals)\n",
    "        acc_performance2.append(test_model(val_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82.88, 88.02, 88.42, 84.02, 88.9, 89.54]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_performance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &      0 &      1 &      2 \\\\\n",
      "\\midrule\n",
      "0 &  82.88 &  88.02 &  88.42 \\\\\n",
      "1 &  84.02 &  88.90 &  89.54 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perf_table = np.array(acc_performance2).reshape(2,3)\n",
    "print(pd.DataFrame(perf_table).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
